{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd7f92c8-c19f-4c6c-a5cd-e0a9bfee98a5",
   "metadata": {},
   "source": [
    "## Finetuning TII's Falcon-7B Model on Intel Max Series GPUs üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b60229a-c6c5-45f8-8136-9f33e73c3cdc",
   "metadata": {},
   "source": [
    "In this notebook, we will be fine-tuning TII's Falcon-7B foundational model. üí™\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f2282-1206-468a-8688-82d31556d88d",
   "metadata": {},
   "source": [
    "#### Step 1: Setting Up the Environment üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19af72a5-9b23-4fe5-8ca8-bf45c7bdf7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08429a0a43114254b2501dd340b41f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2787b702-7414-4ee9-9140-82c8889f02ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import site\n",
    "import os\n",
    "\n",
    "# Install the required packages\n",
    "!{sys.executable} -m pip install --upgrade  \"transformers>=4.38.*\"\n",
    "!{sys.executable} -m pip install --upgrade  \"datasets>=2.18.*\"\n",
    "!{sys.executable} -m pip install --upgrade \"wandb>=0.16.*\"\n",
    "!{sys.executable} -m pip install --upgrade \"trl>=0.7.11\"\n",
    "!{sys.executable} -m pip install --upgrade \"peft>=0.9.0\"\n",
    "!{sys.executable} -m pip install --upgrade \"accelerate>=0.28.*\"\n",
    "\n",
    "# Get the site-packages directory\n",
    "site_packages_dir = site.getsitepackages()[0]\n",
    "\n",
    "# add the site pkg directory where these pkgs are insalled to the top of sys.path\n",
    "if not os.access(site_packages_dir, os.W_OK):\n",
    "    user_site_packages_dir = site.getusersitepackages()\n",
    "    if user_site_packages_dir in sys.path:\n",
    "        sys.path.remove(user_site_packages_dir)\n",
    "    sys.path.insert(0, user_site_packages_dir)\n",
    "else:\n",
    "    if site_packages_dir in sys.path:\n",
    "        sys.path.remove(site_packages_dir)\n",
    "    sys.path.insert(0, site_packages_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52c0da-f19e-4655-8164-33268ad1ab76",
   "metadata": {},
   "source": [
    "We'll now make sure to optimize our environment for the Intel GPU by setting the appropriate environment variables and configuring the number of cores and threads. This will ensure we get the best performance out of our hardware! ‚ö°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff51abb-9ca0-4243-b426-6e6dc407e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "num_physical_cores = psutil.cpu_count(logical=False)\n",
    "num_cores_per_socket = num_physical_cores // 2\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "#HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "# Set the LD_PRELOAD environment variable\n",
    "ld_preload = os.environ.get(\"LD_PRELOAD\", \"\")\n",
    "conda_prefix = os.environ.get(\"CONDA_PREFIX\", \"\")\n",
    "# Improve memory allocation performance, if tcmalloc is not available, please comment this line out\n",
    "os.environ[\"LD_PRELOAD\"] = f\"{ld_preload}:{conda_prefix}/lib/libtcmalloc.so\"\n",
    "# Reduce the overhead of submitting commands to the GPU\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "# reducing memory accesses by fusing SDP ops\n",
    "os.environ[\"ENABLE_SDP_FUSION\"] = \"1\"\n",
    "# set openMP threads to number of physical cores\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_physical_cores)\n",
    "# Set the thread affinity policy\n",
    "os.environ[\"OMP_PROC_BIND\"] = \"close\"\n",
    "# Set the places for thread pinning\n",
    "os.environ[\"OMP_PLACES\"] = \"cores\"\n",
    "\n",
    "print(f\"Number of physical cores: {num_physical_cores}\")\n",
    "print(f\"Number of cores per socket: {num_cores_per_socket}\")\n",
    "print(f\"OpenMP environment variables:\")\n",
    "print(f\"  - OMP_NUM_THREADS: {os.environ['OMP_NUM_THREADS']}\")\n",
    "print(f\"  - OMP_PROC_BIND: {os.environ['OMP_PROC_BIND']}\")\n",
    "print(f\"  - OMP_PLACES: {os.environ['OMP_PLACES']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eeb60a-38e8-4c8e-9d92-57c491588f83",
   "metadata": {},
   "source": [
    "___\n",
    "#### Step 2: Initializing the XPU and monitoring GPU memory in realtime üéÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577cc356-6cec-411e-b258-22d0e7eb9202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>XPU (Intel(R) Data Center GPU Max 1100) :: Memory: Reserved=44.902 GB, Allocated=26.488 GB, Max Reserved=47.34 GB, Max Allocated=44.153 GB</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import threading\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache()\n",
    "    \n",
    "    def get_memory_usage():\n",
    "        memory_reserved = round(torch.xpu.memory_reserved() / 1024**3, 3)\n",
    "        memory_allocated = round(torch.xpu.memory_allocated() / 1024**3, 3)\n",
    "        max_memory_reserved = round(torch.xpu.max_memory_reserved() / 1024**3, 3)\n",
    "        max_memory_allocated = round(torch.xpu.max_memory_allocated() / 1024**3, 3)\n",
    "        return memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated\n",
    "   \n",
    "    def print_memory_usage():\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        print(f\"XPU Name: {device_name}\")\n",
    "        memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "        memory_usage_text = f\"XPU Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "        print(f\"\\r{memory_usage_text}\", end=\"\", flush=True)\n",
    "    \n",
    "    async def display_memory_usage(output):\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        output.update(HTML(f\"<p>XPU Name: {device_name}</p>\"))\n",
    "        while True:\n",
    "            memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "            memory_usage_text = f\"XPU ({device_name}) :: Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "            output.update(HTML(f\"<p>{memory_usage_text}</p>\"))\n",
    "            await asyncio.sleep(5)\n",
    "    \n",
    "    def start_memory_monitor(output):\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        loop.create_task(display_memory_usage(output))\n",
    "        thread = threading.Thread(target=loop.run_forever)\n",
    "        thread.start()    \n",
    "    output = display(display_id=True)\n",
    "    start_memory_monitor(output)\n",
    "else:\n",
    "    print(\"XPU device not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89c13a-d922-4df0-a334-9db67126e292",
   "metadata": {},
   "source": [
    "___\n",
    "#### Step 3: Loading the Model ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fef5d77e-e2b4-4944-9906-655a1b030da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1793a95345c4d9da5dac5a70de3a47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4083de6ccb7b4dd88dfbb8e460041d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7fada7dcf34a1b93d0e06a5494c198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7430253c5a724409a03a96becd27364a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbc7de38e7c4f27a53a32e9b89ee1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68893f433a9f4922a1340616a9f04d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8940f0b206fb48059799ad51add483ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0708034383664ae7bef6b3292625d610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e64d4c1c6374912b7724c583c3e57cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79dde9fa4cf478ea7838b99e77254de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "USE_CPU = False\n",
    "device = \"xpu:0\" if torch.xpu.is_available() else \"cpu\"\n",
    "if USE_CPU:\n",
    "    device = \"cpu\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Set padding side to the right to ensure proper attention masking during fine-tuning\n",
    "tokenizer.padding_side = \"right\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "# Disable caching mechanism to reduce memory usage during fine-tuning\n",
    "model.config.use_cache = False\n",
    "# Configure the model's pre-training tensor parallelism degree to match the fine-tuning setup\n",
    "model.config.pretraining_tp = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca28ce-12b7-42dc-afba-e90c65b029c8",
   "metadata": {},
   "source": [
    "___\n",
    "#### Step 4: Configuring the LoRA Settings üéõÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0876949a-2abc-482e-beed-01ec20eace82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38530297-4387-4420-98cc-fd9e9c2ba59a",
   "metadata": {},
   "source": [
    "___\n",
    "#### Step 5: Testing the Model üß™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91cd310c-0e10-4d5b-be0e-5baca18028ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)    \n",
    "    outputs = model.generate(input_ids, max_new_tokens=100,\n",
    "                             eos_token_id=tokenizer.eos_token_id)    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def test_model(model, test_inputs):\n",
    "    \"\"\"quickly test the model using queries.\"\"\"\n",
    "    for input_text in test_inputs:\n",
    "        print(\"__\"*25)\n",
    "        generated_response = generate_response(model, input_text)\n",
    "        print(f\"{input_text}\")\n",
    "        print(f\"Generated Answer: {generated_response}\\n\")\n",
    "        print(\"__\"*25)\n",
    "\n",
    "test_inputs = [\n",
    "    \"What are the main differences between a vegetarian and a vegan diet?\",\n",
    "    \"What are some effective strategies for managing stress and anxiety?\",\n",
    "    \"Can you explain the concept of blockchain technology in simple terms?\",\n",
    "    \"What are the key factors that influence the price of crude oil in global markets?\",\n",
    "    \"When did Virgin Australia start operating?\"\n",
    "]\n",
    "\n",
    "print(\"Testing the model before fine-tuning:\")\n",
    "test_model(model, test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9c6f2-805d-4ac4-8838-e545b99aa41b",
   "metadata": {},
   "source": [
    "___\n",
    "#### Step 6: Preparing the Dataset üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5abd78e8-d0b8-4709-8655-3239d8238335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 20:12:08,081 - datasets - INFO - PyTorch version 2.1.0.post0+cxx11.abi available.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "print(f\"Instruction is: {dataset[0]['instruction']}\")\n",
    "print(f\"Response is: {dataset[0]['response']}\")\n",
    "\n",
    "# Filter only question Answers\n",
    "categories_to_keep = [\"close_qa\", \"open_qa\", \"general_qa\"]\n",
    "filtered_dataset = dataset.filter(lambda example: example['category'] in categories_to_keep)\n",
    "\n",
    "print(f\"Number of examples in the filtered dataset: {len(filtered_dataset)}\")\n",
    "print(f\"Categories in the filtered dataset: {filtered_dataset['category'][:10]}\")\n",
    "\n",
    "# Remove unwanted fields from the filtered dataset\n",
    "dataset = filtered_dataset.remove_columns([\"context\", \"category\"])\n",
    "print(f\"Number of examples in the dataset: {len(dataset)}\")\n",
    "print(f\"Fields in the dataset: {list(dataset.features.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f040aea3-1d9d-43cf-97e1-a927efd85ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompts(batch):\n",
    "    formatted_prompts = []\n",
    "    for instruction, response in zip(batch[\"instruction\"], batch[\"response\"]):\n",
    "        prompt = f\"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "        formatted_prompts.append(prompt)\n",
    "    return {\"text\": formatted_prompts}\n",
    "\n",
    "dataset = dataset.map(format_prompts, batched=True)\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=99)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "validation_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7eba4-7b66-431c-9af4-8b8f6dd3faa5",
   "metadata": {},
   "source": [
    "___\n",
    "#### Step 7: Finetuning the Model üèãÔ∏è‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5436aece-12d3-4912-8173-06f0c2b35ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51efe90fc2224517a69d5324bd5ba7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2300 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c753b60ecd4264ad371c94bed50c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "2024-06-02 20:12:38,493 - wandb.jupyter - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "ERROR: ld.so: object '/opt/intel/oneapi/intelpython/lib/libtcmalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meduand-alvarez\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "ERROR: ld.so: object '/opt/intel/oneapi/intelpython/lib/libtcmalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/intel/oneapi/intelpython/lib/libtcmalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/uad6b15e0ae3d5e407195ab5f044a50f/workshops/xpu_finetuning/wandb/run-20240602_201239-jq5lh4u5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eduand-alvarez/falcon7b/runs/jq5lh4u5' target=\"_blank\">eduardo-alvarez/falcon-7B-dolly-sft-LoRA</a></strong> to <a href='https://wandb.ai/eduand-alvarez/falcon7b' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eduand-alvarez/falcon7b' target=\"_blank\">https://wandb.ai/eduand-alvarez/falcon7b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eduand-alvarez/falcon7b/runs/jq5lh4u5' target=\"_blank\">https://wandb.ai/eduand-alvarez/falcon7b/runs/jq5lh4u5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='593' max='593' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [593/593 17:49, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.173100</td>\n",
       "      <td>2.116726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.129000</td>\n",
       "      <td>2.066212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.062200</td>\n",
       "      <td>1.986853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.977700</td>\n",
       "      <td>1.942249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.968300</td>\n",
       "      <td>1.938298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: ld.so: object '/opt/intel/oneapi/intelpython/lib/libtcmalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./eduardo-alvarez/falcon-7B-dolly-sft-LoRA/checkpoint-500)... Done. 0.2s\n",
      "ERROR: ld.so: object '/opt/intel/oneapi/intelpython/lib/libtcmalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import wandb\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"falcon7b\" # add wandb project name \n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "os.environ[\"IPEX_TILE_AS_DEVICE\"] = \"1\"\n",
    "\n",
    "finetuned_model_id = \"eduardo-alvarez/falcon-7B-dolly-sft-LoRA\" # change to your info\n",
    "PUSH_TO_HUB = True\n",
    "USE_WANDB = True\n",
    "\n",
    "# Calculate max_steps based on the subset size\n",
    "num_train_samples = len(train_dataset)\n",
    "batch_size = 2\n",
    "gradient_accumulation_steps = 4\n",
    "steps_per_epoch = num_train_samples // (batch_size * gradient_accumulation_steps)\n",
    "num_epochs = 1\n",
    "max_steps = steps_per_epoch * num_epochs\n",
    "print(f\"Finetuning for max number of steps: {max_steps}\")\n",
    "\n",
    "def print_training_summary(results):\n",
    "    print(f\"Time: {results.metrics['train_runtime']: .2f}\")\n",
    "    print(f\"Samples/second: {results.metrics['train_samples_per_second']: .2f}\")\n",
    "    get_memory_usage()\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_ratio=0.05,\n",
    "        max_steps=max_steps,\n",
    "        learning_rate=1e-5,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_steps=500,\n",
    "        bf16=True,\n",
    "        logging_steps=100,\n",
    "        output_dir=finetuned_model_id,\n",
    "        hub_model_id=finetuned_model_id if PUSH_TO_HUB else None,\n",
    "        use_ipex=True,\n",
    "        report_to=\"wandb\" if USE_WANDB else None,\n",
    "        push_to_hub=PUSH_TO_HUB,\n",
    "        max_grad_norm=0.6,\n",
    "        weight_decay=0.01,\n",
    "        group_by_length=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=True\n",
    ")\n",
    "\n",
    "if device != \"cpu\":\n",
    "    print_memory_usage()\n",
    "    torch.xpu.empty_cache()\n",
    "results = trainer.train()\n",
    "print_training_summary(results)\n",
    "#wandb.finish()\n",
    "\n",
    "# save lora model\n",
    "tuned_lora_model = \"falcon-7B-dolly-sft-LoRA\" # change to your info\n",
    "trainer.model.save_pretrained(tuned_lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f1be9-87dc-4b1c-848d-eb1190bdb194",
   "metadata": {},
   "source": [
    "___\n",
    "#### Step 8: Save the Finetuned Model üíæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b981a9-e0f7-4f7d-bc59-a3f54f51a772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af77484a4417422d9dde8b059a5b2ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "tuned_model = \"falcon-7B-dolly-sft-LoRA\" # change to your info\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, tuned_lora_model)\n",
    "model = model.merge_and_unload()\n",
    "# save final tuned model\n",
    "model.save_pretrained(tuned_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d256f58-628c-4f00-8a2a-42bab3b99fe4",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### Disclaimer for Using Large Language Models\n",
    "\n",
    "Please be aware that while Large Language Models like Camel-5B and OpenLLaMA 3b v2 are powerful tools for text generation, they may sometimes produce results that are unexpected, biased, or inconsistent with the given prompt. It's advisable to carefully review the generated text and consider the context and application in which you are using these models.\n",
    "\n",
    "Usage of these models must also adhere to the licensing agreements and be in accordance with ethical guidelines and best practices for AI. If you have any concerns or encounter issues with the models, please refer to the respective model cards and documentation provided in the links above.\n",
    "\n",
    "To the extent that any public or non-Intel datasets or models are referenced by or accessed using these materials those datasets or models are provided by the third party indicated as the content source. Intel does not create the content and does not warrant its accuracy or quality. By accessing the public content, or using materials trained on or with such content, you agree to the terms associated with that content and that your use complies with the applicable license.\n",
    "\n",
    "Intel expressly disclaims the accuracy, adequacy, or completeness of any such public content, and is not liable for any errors, omissions, or defects in the content, or for any reliance on the content. Intel is not liable for any liability or damages relating to your use of public content.\n",
    "\n",
    "Intel‚Äôs provision of these resources does not expand or otherwise alter Intel‚Äôs applicable published warranties or warranty disclaimers for Intel products or solutions, and no additional obligations, indemnifications, or liabilities arise from Intel providing such resources. Intel reserves the right, without notice, to make corrections, enhancements, improvements, and other changes to its materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
